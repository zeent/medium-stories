{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c6133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Load data from the CSV file\n",
    "data = pd.read_csv('data/DS-2023-000/proxy-sql-dataset.csv', delimiter=\";\")\n",
    "\n",
    "# Convert text labels to numbers (0 or 1)\n",
    "label_encoder = LabelEncoder()\n",
    "data['malignant'] = label_encoder.fit_transform(data['malignant'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['query'], \n",
    "                                                    data['malignant'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Tokenization and padding of text\n",
    "tokenizer = Tokenizer(num_words=5000, \n",
    "                      oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=50, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "num_words = 5000\n",
    "output_dim = 16\n",
    "filters = 128\n",
    "kernel_size = 5\n",
    "dense_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffbe2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0601 - accuracy: 0.9772 - val_loss: 0.0161 - val_accuracy: 0.9961\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.0149 - val_accuracy: 0.9964\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.0140 - val_accuracy: 0.9964\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0089 - accuracy: 0.9984 - val_loss: 0.0156 - val_accuracy: 0.9960\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.0159 - val_accuracy: 0.9962\n",
      "250/250 [==============================] - 0s 495us/step - loss: 0.0159 - accuracy: 0.9962\n",
      "Loss: 0.015937848016619682, Accuracy: 0.9962499737739563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construeix el model de CNN\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=output_dim, input_length=50),\n",
    "    Conv1D(filters, kernel_size, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(dense_units, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenament del model\n",
    "model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Avaluació del model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b360a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t100   \n",
      "1  \t78    \n",
      "2  \t72    \n",
      "3  \t73    \n",
      "4  \t72    \n",
      "5  \t77    \n",
      "6  \t81    \n",
      "7  \t77    \n",
      "8  \t73    \n",
      "9  \t75    \n",
      "10 \t90    \n",
      "Best Individual: [1786, 17, 202, 5, 68]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "# Definim el problema d'optimizació (maximitzar la precisió)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Funció d'avaluació (fitness)\n",
    "# Aquest codi crea el model a partir del nostre individu (individual)\n",
    "# i n'avalua la precisió (accuracy)\n",
    "def evaluate(individual, X_train, y_train, X_test, y_test):\n",
    "    # Transformem l'individu en paràmetres de la CNN \n",
    "    num_words, output_dim, filters, kernel_size, dense_units = individual\n",
    "\n",
    "    # Construim el model\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=num_words, output_dim=output_dim, input_length=50),\n",
    "        Conv1D(filters, kernel_size, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compilem el model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Tokenització i padding del text\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=50, padding='post', truncating='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "    # Entrenament del model\n",
    "    model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_test_padded, y_test), verbose=0)\n",
    "\n",
    "    # Avaluació del model\n",
    "    _, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "    return accuracy,\n",
    "\n",
    "# Funció per generar individus aleatoris\n",
    "def init_individual():\n",
    "    num_words = random.randint(100, 5000)\n",
    "    output_dim = random.randint(8, 32)\n",
    "    filters = random.randint(32, 256)\n",
    "    kernel_size = random.randint(3, 10)\n",
    "    dense_units = random.randint(32, 128)\n",
    "    return [num_words, output_dim, filters, kernel_size, dense_units]\n",
    "\n",
    "# Configuració de l'evolució\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, init_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=[100, 8, 32, 3, 32], up=[5000, 32, 256, 10, 128], indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Divisió de les dades d'entrenament\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['query'], data['malignant'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuració de la població\n",
    "population_size = 100\n",
    "generations = 10\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Algorisme Genètic Simple\n",
    "algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=generations, stats=None, halloffame=None, verbose=True)\n",
    "\n",
    "# Millor individu després de l'evolució\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "print(\"Best Individual:\", best_individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70454b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best individual num_words = 1786\n",
      "Best individual output_dim = 17\n",
      "Best individual filters = 202\n",
      "Best individual kernel_size = 5\n",
      "Best individual dense_units = 68\n"
     ]
    }
   ],
   "source": [
    "(   gao_num_words, \n",
    "    gao_output_dim, \n",
    "    gao_filters, \n",
    "    gao_kernel_size, \n",
    "    gao_dense_units\n",
    ") = best_individual\n",
    "\n",
    "\n",
    "print(\"Best individual num_words = {}\".format(gao_num_words))\n",
    "print(\"Best individual output_dim = {}\".format(gao_output_dim))\n",
    "print(\"Best individual filters = {}\".format(gao_filters))\n",
    "print(\"Best individual kernel_size = {}\".format(gao_kernel_size))\n",
    "print(\"Best individual dense_units = {}\".format(gao_dense_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0daa267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0619 - accuracy: 0.9789 - val_loss: 0.0185 - val_accuracy: 0.9958\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0127 - accuracy: 0.9974 - val_loss: 0.0153 - val_accuracy: 0.9967\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.0150 - val_accuracy: 0.9970\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0142 - val_accuracy: 0.9965\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.0143 - val_accuracy: 0.9966\n",
      "250/250 [==============================] - 0s 500us/step - loss: 0.0143 - accuracy: 0.9966\n",
      "Loss: 0.014347785152494907, Accuracy: 0.9966250061988831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construeix el model de CNN\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=output_dim, input_length=50),\n",
    "    Conv1D(filters, kernel_size, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(dense_units, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenament del model\n",
    "model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Avaluació del model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dba0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
